# Unsupervised Fraud Detection: A Multi-Method Approach to Identifying Anomalous Transactions


### Team: Tuan Anh Dang, Matthew Gin, Sam Goldberg, Matthew Pilc  
## Introduction

  The overarching goal of this project is to explore and build different machine learning methods in order to most accurately predict fraudulent transactions using a variety of predictor variables. After initial exploratory data analysis, we explored a variety of methods including K-means clustering, DBSCAN, Gaussian Mixture Models (GMMs), and a synergizing Ensemble Method. We executed these clustering models both on the raw data as well as data dimension-reduced by PCA, with the intention of decreasing the “curse of dimensionality” and potentially improving the models’ clustering ability. With the findings of these models, we complete our exploration by identifying which entries were most suspected of being fraudulent.
	The dataset we used was called “Bank Transaction Dataset for Fraud Detection,” which we found on Kaggle. The dataset included 2,512 samples of bank transaction data and included variables on various transaction attributes, demographics, and customer usage patterns. One aspect of the dataset that guided the direction of this project was the fact that there was no variable indicating whether a transaction was fraudulent or not. With this in mind, we focused mainly on building clustering methods instead of classification methods such as logistic regression.

## Exploratory Data Analysis:
  In our exploratory data analysis, we began by enhancing the dataset through feature engineering. We created a new feature, time_between_transactions, calculated as the difference between TransactionDate and PreviousTransactionDate. For some reason (likely a mislabel in the dataset), TransactionDate occurred before PreviousTransactionDate in the dataset. To fix this, we multiplied the resulting values by -1 to ensure all values were positive and meaningful. To improve data quality, we removed duplicate rows and eliminated negative values from columns such as TransactionAmount, TransactionDuration, and AccountBalance where they were not logical. 
  Univariate analysis allowed us to explore individual variables. Histograms and kernel density plots highlighted key distributions, such as the right-skewed nature of TransactionAmount and AccountBalance, indicating the presence of high-value outliers. LoginAttempts was predominantly centered around one, with occasional spikes hinting at unusual behavior. Boxplots further emphasized the presence of outliers, particularly in TransactionAmount.
  In our bivariate analysis, we examined relationships between features. A correlation heatmap revealed generally weak relationships among numeric variables, although we observed a slight positive correlation between CustomerAge and AccountBalance. Scatterplots provided additional insights, such as the association between high TransactionAmount values and shorter time_between_transactions. Boxplots also showed how TransactionAmount varied significantly across TransactionType and Channel, with online transactions often involving higher amounts.
  For categorical variables, we visualized their distributions using count plots. We observed that most transactions were of the “Debit” type, with fewer “Credit” transactions. Boxplots highlighted how TransactionAmount varied across transaction types and channels, while a scatterplot of TransactionAmount against time_between_transactions identified potential anomalies, such as large transactions conducted in quick succession.
  In our multivariate analysis, we combined key features together into one visualization. A 3D scatterplot of TransactionAmount, AccountBalance, and TransactionDuration revealed clusters of transactions differentiated by CustomerAge, with younger customers tending to have smaller balances and lower transaction amounts. A heatmap created from a pivot table of average TransactionAmount by TransactionType and Location showed us geographic trends. We noticed that certain locations consistently had higher transaction values.

## Fraud Detection

### Data Preparation:
  We began by normalizing numeric columns such as TransactionAmount, TransactionDuration, and AccountBalance using standard scaling, ensuring all features contributed equally to clustering. Categorical variables were one-hot encoded and scaled for consistency with numeric features. However, for simplicity and to avoid over-representing categorical variables in clustering algorithms, we focused primarily on normalized numeric data (scaled_bank_data) for certain methods. This preprocessing laid the foundation for the application of unsupervised learning techniques.

### PCA for Dimension Reduction:
  We acknowledge that our dataset represented a large amount of bank information in relatively high-dimensional space. For machine learning, we know the “curse of dimensionality” can present severe data complications, meaning when data starts to enter increasingly high-dimensionality, it becomes exponentially harder to capture interpretable representations of the information as distances between points lose meaning. Because of this, we decided to implement Principal Component Analysis (PCA) as a statistical technique to reduce the dimensionality of the dataset while retaining much of the information (in our case, at least 70%). During our exploration, we found that four (4) principal components were able to capture 72.8% of the variance in our dataset. We later use PCA dimension-reduced data on our models.

### K-Means Clustering (using original data):
  Using K-means clustering, we partitioned the dataset into three clusters (n_clusters=3). Each transaction was assigned a cluster based on proximity to its cluster centroid, measured by Euclidean distance. To identify potential fraud, we calculated the distance of each point to its assigned cluster centroid and flagged transactions as anomalous if their distance exceeded the mean by three standard deviations. This method identified 71 potentially fraudulent transactions, with notable cases involving unusually high login attempts (4-5) and large transaction amounts.
  Visualizations, such as scatterplots of TransactionAmount vs. LoginAttempts and TransactionAmount vs. time_between_transactions, highlighted anomalies (marked in red) and provided insights into their clustering patterns. Transactions with excessive login attempts or unusually rapid transaction sequences were particularly flagged, aligning with expected fraud indicators.

### K-Means Clustering on PCA-Reduced Data:
  We reapplied K-means clustering on the PCA-reduced data, increasing the number of clusters to four (n_clusters=4) to better capture the structure of the transformed dataset. Similar to the previous clustering step, we calculated distances to centroids and identified anomalies using the mean-plus-three-standard-deviations threshold. This approach flagged 64 potentially fraudulent transactions, slightly refining the results compared to clustering on raw scaled data.
  By reducing dimensionality, we improved the interpretability of clustering results. A scatterplot of TransactionAmount vs. LoginAttempts on PCA-reduced clusters revealed patterns similar to those observed previously but with improved separation between clusters. This confirmed that dimension reduction enhanced the clustering process without sacrificing key fraud detection signals.

### DBSCAN Analysis
  The Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm was utilized to identify clusters in the dataset. DBSCAN is a density-based unsupervised clustering method that groups points into clusters based on their spatial proximity and density while classifying low-density points as noise. This characteristic makes it particularly effective for datasets with irregularly shaped clusters or significant noise.
  In this study, the key hyperparameters of DBSCAN—eps (the maximum distance between two points to be considered neighbors) and min_samples (the minimum number of points required to form a dense cluster)—were carefully tuned to achieve optimal clustering results. The elbow method, a common technique for identifying optimal hyperparameters, was applied to select an appropriate value for eps. Specifically, the k-nearest neighbors (kNN) plot was employed, where the distances of points to their k-th nearest neighbor were calculated and plotted in ascending order. The elbow point, identified as the point of maximum curvature in the kNN plot, provided a robust estimate for eps. This approach ensured that clusters were neither too fragmented nor too large, allowing for meaningful and interpretable results.
  The analysis demonstrated that DBSCAN effectively identified dense clusters within the data, distinguishing them from low-density regions that were classified as noise. The ability of DBSCAN to detect and label outliers was particularly useful for anomaly detection. However, the sensitivity of the results to the choice of eps and min_samples necessitated careful tuning to avoid over-clustering or under-clustering. The combination of the elbow method and domain knowledge helped mitigate these challenges and enhance the algorithm’s performance.
  DBSCAN’s density-based approach was especially effective for the given dataset, where clusters exhibited nonlinear structures. By identifying both dense regions and isolated points, DBSCAN provided valuable insights into the underlying structure of the data.

### Gaussian Mixture Model (GMM) Analysis
  Next, we applied the Gaussian Mixture Model (GMM) to analyze the dataset using a probabilistic clustering approach. GMM assumes that the data is generated from a mixture of Gaussian distributions and assigns probabilities to each point based on its likelihood of belonging to a given cluster. This flexibility enables the model to identify clusters with complex covariance structures.
To prepare the data for GMM, Principal Component Analysis (PCA) was performed to reduce the dimensionality while preserving the most important features. This preprocessing step improved the efficiency of the clustering process and mitigated potential issues caused by high dimensionality. The GMM parameters were estimated using the Expectation-Maximization (EM) algorithm, which iteratively maximizes the likelihood of the observed data under the model.
  The results showed that GMM effectively segmented the data into clusters, with each cluster characterized by a unique mean and covariance structure. Anomalies were identified by calculating Mahalanobis distances, which measure the distance of each point from the cluster centers while accounting for the shape of the clusters. Points with distances exceeding a threshold, defined as the mean plus three standard deviations, were flagged as potential outliers. This approach provided a probabilistic framework for anomaly detection, complementing the deterministic nature of DBSCAN.
  The use of PCA in conjunction with GMM not only improved computational efficiency but also facilitated the visualization of clusters in a reduced-dimensional space. This enhanced interpretability, making it easier to identify and understand patterns within the data.

### Synergizing With Ensemble Learning Method
  We leverage our foundational understanding of ensemble methods, or approaches that combine multiple simple "building block" models in an attempt to obtain a single, well performing, model. We refer to our K-Nearest Neighbor, DBSCAN, and Gaussian Mixture Model as weak learners, as we do not expect them to result in particularly accurate predictions by themselves. Because we have identified that our data has high dimensionality, we wanted to compose a more robust anomaly detector for fraud by combining our k-means, DBSCAN, and Gaussian Mixture Model in an ensemble learning method so we can combine the predictions of multiple methods in attempt to improve our overall performance in identifying fraudulent activity and make our predictions more robust. We understand that fraud detection data includes noisy and potentially imbalanced observations. However, combining all three unsupervised learning methods should result in more robust and accurate fraud detection.
  Our ensemble learning model leverages a kNN non parametric model to identify local patterns in the data while being sensitive to noise. Given our data, we do not know the conditional distribution of our response Y given our input X, therefore, we wanted to utilize our kNN method to identify the K points in our training data that are closest to X0. Our DBSCAN algorithm performs another form of clustering, based on the data's spatial proximity and density when trying to classify lower density observations as noise. The results from the DBSCAN anomaly visualization for Login Attempts, for example, show that the density clustering algorithm identifies fraud instances with attempts ranging from 1 to 5. Combining this with the kNN clustering algorithm provides a more robust prediction space than one model. Finally, our GMM analysis handles softer probabilistic clusters but assumes Gaussian distribution for. We combine kNN, DBSCAN, and GMM to develop an ensemble, majority vote, learning algorithm–requiring that two out of the three models classify an observation as fraud for it to be classified as fraud by the ensemble.



### Identifying the Top N Most Suspicious Transactions:
To refine our fraud detection process, we implemented a method to calculate the fraud likelihood of each transaction. This likelihood score combines the outputs of three clustering methods: K-Means, Gaussian Mixture Model (GMM), and DBSCAN. By integrating these techniques, we aimed to leverage their complementary strengths and produce a robust measure of suspiciousness for each transaction.

### Fraud Likelihood Calculation
  We calculated the fraud likelihood score as a weighted sum of normalized scores from K-Means, GMM, and DBSCAN. This approach ensured that each method contributed proportionally to the final score:

  To calculate our K-Means score, for each transaction, we computed its distance to the assigned cluster centroid and normalized this value by dividing it by the maximum distance across all transactions. This normalized distance reflects how anomalous a transaction is within its cluster. To calculate the GMM score, we calculated the Mahalanobis distance of each transaction to its cluster center as determined by the Gaussian Mixture Model. This value was also normalized to make it comparable to the other methods. For our DBSCAN score, we knew that DBSCAN identifies anomalies as binary labels (1 for fraud, 0 for non-fraud), so we directly used these labels as actual the DBSCAN score.
  
  To determine the overall fraud likelihood, we applied weights to the normalized scores from each method. Based on the scoring methods we used, we decided to assign equal weights to K-Means and GMM (40% each) and a smaller weight to DBSCAN (20%), due to the discrete nature of the DBSCAN scores.
  Using the fraud likelihood scores, we ranked all transactions and identified the top 10 most suspicious ones. These transactions exhibited either high distances to their cluster centroids, DBSCAN anomaly flags, or a combination of both. The table below highlights the top 10 suspicious transactions, showing key details such as the transaction amount, login attempts, account balance, time between transactions, and the calculated fraud likelihood score.

### Analysis of Vulnerable Accounts:
  We further analyzed transactions with a fraud likelihood score greater than 50% to identify vulnerable accounts. The results revealed that certain accounts, such as AC00071 and AC00358, appeared multiple times among the flagged transactions, indicating potentially fraudulent behavior concentrated within specific accounts. Analysis like this can have real-world applications, as banks can use it to guide targeted investigations on accounts and transactions, or provide preventative measures to limit fraud in the future.


